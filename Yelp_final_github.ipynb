{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Thai Yelp Mythbuster and Food Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and EDA - Overview Data-Set (restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data-set of top restaurants with the reviews as a single string(overview yelp-data set; from BeautifulSoup)\n",
    "df=pd.read_csv(\"./yelp_sfo_thai.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysing distribution of Thai restaurant rating in SF\n",
    "rating=df[\"rating\"]\n",
    "sns.set(style=\"white\",palette=\"muted\",color_codes=True)\n",
    "ax=sns.countplot(rating)\n",
    "ax.set_title(\"Distribution of Rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysing distribution of number of reviews per Thai restaurant in SF\n",
    "reviews=df[\"review_count\"]\n",
    "sns.set(style=\"white\",palette=\"muted\",color_codes=True)\n",
    "ax=sns.distplot(reviews)\n",
    "ax.set_title(\"Distribution of Number of Reviews\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the distribution of Thai price range in SF\n",
    "price_map={\"$\":\"level_1\",\"$$\":\"level_2\",\"$$$\":\"level_3\",\"$$$$\":\"level_4\"}\n",
    "price=df.price.map(price_map)\n",
    "sns.set(style=\"white\",palette=\"muted\",color_codes=True)\n",
    "ax=sns.countplot(price)\n",
    "ax.set_title(\"Distribution of price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.rename(columns={'Name':'name'})\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the reviews data set from Beautiful Soup\n",
    "df2 = pd.read_csv('./yelp_sfo_thai_reviews_again.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(data=df2, columns=['name','reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging both the data sets\n",
    "df=df.merge(df2,on='name',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing sentiment analysis packages from NLTK\n",
    "# Vader SentimentAnalyzer was used to obtain the polarity scores for the reviews of restaurants. \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysing 'reviews'\n",
    "df['reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appending Sentiments Scores to the restaurants\n",
    "sentiments = []\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for i in range(df.shape[0]):\n",
    "        line = df['reviews'].iloc[i]\n",
    "        sentiment = sid.polarity_scores(line)\n",
    "        sentiments.append([sentiment['neg'], sentiment['pos'],\n",
    "                           sentiment['neu'], sentiment['compound']])\n",
    "df[['neg', 'pos', 'neu', 'compound']] = pd.DataFrame(sentiments)\n",
    "#df['Negative'] = df['compound'] < -0.1\n",
    "#df['Positive'] = df['compound'] > 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The rating is positively correlated with the positive sentiment and negatively correlated with the negative sentiment. The Thai restaurant  can reasonably represent the perception of a restaurant as is expressed in the reviews. However, the correlation between review count and rating is inconclusive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and EDA - master(granular) data-set (individual reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data-set of all the restaurants with the reviews(granular yelp-data set; from scrapy)\n",
    "df = pd.read_csv('./yelp/yelp_thai_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.to_series().groupby(df.dtypes).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'] = df['price'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['restaurant', 'address']).agg({'text': ['count'], 'rating': ['mean', 'std']}).sort_values(('text', 'count'), ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyzing the scatter distribution between rating v/s price\n",
    "plt.scatter(df['price'], df['rating'])\n",
    "plt.xlabel('price')\n",
    "plt.ylabel('rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[df.price==3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of ratings(frequency)\n",
    "df['rating'].plot(kind='hist')\n",
    "plt.xlabel('rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the linear relation b/w rating and price - inconclusive\n",
    "sns.lmplot(\"price\", \"rating\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis on reviews for EDA\n",
    "sentiments = []\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for i in range(df.shape[0]):\n",
    "        line = df['text'].iloc[i]\n",
    "        sentiment = sid.polarity_scores(line)\n",
    "        sentiments.append([sentiment['neg'], sentiment['pos'],\n",
    "                           sentiment['neu'], sentiment['compound']])\n",
    "df[['neg', 'pos', 'neu', 'compound']] = pd.DataFrame(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the linear relation b/w rating and pos_sentiment - positive correlation\n",
    "sns.lmplot(\"rating\", \"pos\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the linear relation b/w price and pos_sentiment - inconclusive\n",
    "sns.lmplot(\"price\", \"pos\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the linear relation b/w rating and neg_sentiment - negative correlation\n",
    "sns.lmplot(\"rating\", \"neg\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the linear relation b/w price and neg_sentiment - inconclusive\n",
    "sns.lmplot(\"price\", \"neg\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysing box-plot of ratings which show a bulk of ratings b/w 3 and 5\n",
    "sns.boxplot(x=df['rating'],  data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Classification Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We perform a classification modelling to check if we can predict a good/bad rating based on a review as a baseline before sentiment as the reviews contain all of our target(food) tokens. We vectorize the reviews using Count Vectorizer and TFIDF Vectorizer and run a Logistic Regression and a Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting value counts on the ratings\n",
    "df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a binary classifier for rating>=4 as good, rating<4 as bad \n",
    "df['binary_rating'] = df['rating'].apply(lambda x:1 if x==4 or x==5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for class imbalance\n",
    "df['binary_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting our X(feature) and y(target)\n",
    "X = df['text']\n",
    "y = df['binary_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprossing for modelling\n",
    "# import string\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# def text_process(text):\n",
    "#     '''\n",
    "#     Takes in a string of text, then performs the following:\n",
    "#     1. Remove all punctuation\n",
    "#     2. Remove all stopwords\n",
    "#     3. Return the cleaned text as a list of words\n",
    "#     '''\n",
    "#     nopunc = [char for char in text if char not in string.punctuation]\n",
    "#     nopunc = ''.join(nopunc)\n",
    "#     return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words='english').fit(X)\n",
    "tfidf = TfidfVectorizer(stop_words='english').fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cvec.get_feature_names()))\n",
    "print(len(tfidf.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = cvec.transform(X)\n",
    "X2 = tfidf.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X1_train,X1_test,y1_train,y1_test = train_test_split(X1,y,shuffle=True,stratify=y,random_state=42,test_size=0.3)\n",
    "X2_train,X2_test,y2_train,y2_test = train_test_split(X2,y,shuffle=True,stratify=y,random_state=42,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resampling Train Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "# traindata = X_train.to_frame().merge(pd.DataFrame(y_train), how = 'left', right_index = True, left_index = True)\n",
    "# train_majority = traindata[traindata['binary_rating'] == 1]\n",
    "# train_minority = traindata[traindata['binary_rating'] == 0]\n",
    "# train_minority_upsampled = resample(train_minority, \n",
    "#                                      replace = True, \n",
    "#                                      n_samples = train_majority.shape[0],\n",
    "#                                      random_state = 42)\n",
    "\n",
    "# train_data_upsampled = pd.concat([train_majority, train_minority_upsampled])\n",
    "# X_train = train_data_upsampled.drop(columns = 'binary_rating')\n",
    "# y_train = train_data_upsampled['binary_rating']\n",
    "# y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer - Classification Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X1_train, y1_train)\n",
    "y1_pred = lr.predict(X1_test)\n",
    "print('accuracy score',accuracy_score(y1_test, y1_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and Classification Report\n",
    "print(confusion_matrix(y1_test, y1_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y1_test, y1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X1_train, y1_train)\n",
    "y1_pred = nb.predict(X1_test)\n",
    "print('accuracy score',accuracy_score(y1_test, y1_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and Classification Report\n",
    "print(confusion_matrix(y1_test, y1_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y1_test, y1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Vectorizer - Classification Modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X2_train, y2_train)\n",
    "y2_pred = lr.predict(X2_test)\n",
    "print('accuracy score',accuracy_score(y2_test, y2_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and Classification Report\n",
    "print(confusion_matrix(y2_test, y2_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y2_test, y2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X2_train, y2_train)\n",
    "y2_pred = nb.predict(X2_test)\n",
    "print('accuracy score',accuracy_score(y2_test, y2_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and Classification Report\n",
    "print(confusion_matrix(y2_test, y2_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y2_test, y2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Word2Vec Modelling on tokens in the corpus - finding dishes to predict/relate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The goal of word vector embedding models, or word vector models for short, is to learn dense, numerical vector representations for each term in a corpus vocabulary. If the model is successful, the vectors it learns about each term should encode some information about the meaning or concept the term represents, and the relationship between it and other terms in the vocabulary. Word vector models are also fully unsupervised — they learn all of these meanings and relationships solely by analyzing the text of the corpus, without any advance knowledge provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing gensim and nltk packages\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Phrases\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the text corpus as a list of individual reviews\n",
    "reviews_list = list(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the words in the reviews - setting uni-grams\n",
    "sentences = [word_tokenize(x) for x in reviews_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences), len(reviews_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a uni-gram Word2Vec model as a baseline\n",
    "model = gensim.models.Word2Vec(sentences)\n",
    "model.train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Cosine-Similarity for word-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing token similarity with other words in the corpus\n",
    "model.most_similar('chicken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing token similarity with other words in the corpus\n",
    "model.most_similar('soup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing token similarity with other words in the corpus\n",
    "model.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing token similarity with other words in the corpus\n",
    "model.most_similar('bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individually comparing word vectors\n",
    "model.similarity('chicken', 'good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individually comparing word vectors\n",
    "model.similarity('chicken', 'beef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bi-grams from word vectors for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramer = gensim.models.Phrases(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Word2Vec(bigramer[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.most_similar('pad_thai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tri-grams from bi-grams for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = Phrases(bigramer[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Word2Vec(trigram[bigramer[sentences]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.most_similar('pad_thai')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that the cosine similarity gives more accurate results when the unsupervised model incorporates trigrams in the unsupervised machine learning model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing addition of the word vectors to give logical inferences from the unsupervised model(cosine-addition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_algebra(add=[], subtract=[], topn=1):\n",
    "    \"\"\"\n",
    "    combine the vectors associated with the words provided\n",
    "    in add= and subtract=, look up the topn most similar\n",
    "    terms to the combined vector, and print the result(s)\n",
    "    \"\"\"\n",
    "    answers = model3.most_similar(positive=add, negative=subtract, topn=topn)\n",
    "    \n",
    "    for term, similarity in answers:\n",
    "        print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=[u'breakfast', u'lunch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=[u'chicken', u'good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=[u'lunch', u'night'], subtract=[u'day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=[u'noodle', u'burmese'], subtract=[u'rice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=[u'pad_thai', u'thai'], subtract=[u'chicken'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_algebra(add=[u'thai', u'fine_dining'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA modelling\n",
    "#### In NLP applications, documents are represented a mixture of the individual tokens (words and phrases) they contain. There are two layers in this model — documents and tokens — and the size or dimensionality of the document vectors is the number of tokens in the corpus vocabulary. \n",
    "#### Using LDA documents are represented as a mixture of a pre-defined number of topics, and the topics are represented as a mixture of the individual tokens in the vocabulary thereby reducing the dimensionality of the model. \n",
    "#### LDA is fully unsupervised. The topics are \"discovered\" automatically from the data by trying to maximize the likelihood of observing the documents in your corpus, given the modeling assumptions. LDA uses a simplifying assumption known as the bag-of-words model. In the bag-of-words model, a document is represented by the counts of distinct terms that occur within it. \n",
    "#### pyLDAvis takes topic models created with gensim and prepare their data for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### The scope of LDA is reduced to a single restaurant\n",
    "# dr = df[df.restaurant == 'Marnee Thai']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_list_ = list(dr.text)\n",
    "# sentences_    = [word_tokenize(x) for x in reviews_list_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyLDAvis.gensim\n",
    "# from gensim import corpora, models\n",
    "# pyLDAvis.enable_notebook() # in order for our visual to show up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary = corpora.Dictionary(sentences_)\n",
    "\n",
    "# corpus = [dictionary.doc2bow(text) for text in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ldamodel = models.ldamodel.LdaModel(corpus,\n",
    "#                                     id2word = dictionary, # connect each word to its \"spot\" in the dictionary\n",
    "#                                     num_topics = 50, # hyperparameter T for number of topics\n",
    "#                                     passes = 5, # similar to epochs, how many times do we iterate through the data\n",
    "#                                     minimum_probability = 0.01) # only including topics that meet some probability threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE - wordvector visualization with sci-kit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Word2Vec(sentences_, workers=4, size=100, min_count=50, window=10, sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = model[model.wv.vocab]\n",
    "\n",
    "# tsne = TSNE(n_components=2)\n",
    "# X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Modelling to find Best Thai Dishes in SF using ratings/sentiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the granular data-set with all the Thai restaurants and individual reviews\n",
    "df = pd.read_csv('./yelp/yelp_thai_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a master menu based on knowledge of Thai cuisine and based of most menus\n",
    "sample_menu=[\n",
    "'noodle soup','chicken noodle soup','beef noodle soup','noodles','pad see ew','pad kee mao','pad thai','fried rice','salad','papaya salad','papaya','chicken satay','satay','egg rolls','chicken','beef','fried chicken','roast duck','bbq pork','pork','roasted duck','panang curry','green curry','yellow curry','tom yum','tom kha','tom ka','thai iced tea','thai iced coffee',\n",
    "'imperial rolls','angel wings','wings','corn cakes','mango salad','panang beef','curry','basa fillet','tofu','pumpkin curry','coconut ice cream',\n",
    "'eggplant','fried banana','sticky rice','basil','pork belly','silver noodle','crab','calamari','cashew nut','fish cake','fish cakes','peanut sauce','samosa','catfish','pineapple fried rice','puff','money bag','money bags','silver noodle','pad see you','larb','quail','prawns','fried prawn','fried prawns','shrimp','seafood','salmon','ribs','chicken noodle','beef noodle','roti','pad cha','spring rolls','rolls','fried egg','imperial roll','spring roll','egg roll','tuna tower','volcanic beef','sea bass','crab cake',\n",
    "'pad kee mow','massamam','lamb','drunken noodles','mango','coconut'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just rating and review text\n",
    "df = df[['rating', 'text']]\n",
    "\n",
    "#switch to lower case\n",
    "df.text = df.text.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_menu = list(set(sample_menu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking for dishes in sample menu and scoring them based on review-rating based on the average of the frequency they occur in the corpus - Scoring 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns just those reviews that have the word\n",
    "# in the text of the review\n",
    "def subset_reviews(word, df):\n",
    "    return df[df.text.str.contains(word)]\n",
    "\n",
    "# return avg rating of revies that contain dish\n",
    "def avg_review_of_dish(item, df):\n",
    "    return subset_reviews(item, df).mean()\n",
    "\n",
    "# return nuber of times dishes reviewed\n",
    "def dish_count(item, df):\n",
    "    return subset_reviews(item, df).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dish review dataframe\n",
    "dish_ratings = [avg_review_of_dish(item, df)[0] for item in sample_menu]\n",
    "dish_counts = [dish_count(item, df) for item in sample_menu]\n",
    "data = {'dish' : sample_menu,\n",
    "       'rating' : dish_ratings,\n",
    "       'times_reviewed': dish_counts}\n",
    "dish_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dish review dataframe\n",
    "dish_df[dish_df.times_reviewed > 99].sort_values(by = 'rating', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating each review by scoring sentiment on the review for dishe rating - Scoring 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sentiment df which is text, polarity of text\n",
    "from textblob import TextBlob\n",
    "df_sentiment = df[['text']]\n",
    "df_sentiment.text = df_sentiment.text.apply(lambda x: x.lower())\n",
    "df_sentiment['polarity'] = df_sentiment.text.apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dish review dataframe\n",
    "dish_ratings = [avg_review_of_dish(item, df_sentiment)[0] for item in sample_menu]\n",
    "dish_counts = [dish_count(item, df_sentiment) for item in sample_menu]\n",
    "data = {'dish' : sample_menu,\n",
    "       'rating' : dish_ratings,\n",
    "       'times_reviewed': dish_counts}\n",
    "dish_df_sentiment = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dish_df_sentiment[dish_df_sentiment.times_reviewed > 99].sort_values(by = 'rating', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Granularly evaluating each review by scoring sentence sentiment as opposed to review sentiment for dishes - Scoring 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the reviews into sentences\n",
    "reviews_sentences = ''.join(list(df.text)).split('.')\n",
    "\n",
    "# create a dataframe with these sentences\n",
    "data = {'text' : reviews_sentences}\n",
    "sentences_df = pd.DataFrame(data)\n",
    "sentences_df.text = sentences_df.text.apply(lambda x: x.lower())\n",
    "\n",
    "# create column which is polarity of text\n",
    "from textblob import TextBlob\n",
    "sentences_df['polarity'] = sentences_df.text.apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dish_ratings = [avg_review_of_dish(item, sentences_df)[0] for item in sample_menu]\n",
    "dish_counts = [dish_count(item, sentences_df) for item in sample_menu]\n",
    "data = {'dish' : sample_menu,\n",
    "       'rating' : dish_ratings,\n",
    "       'times_mentioned': dish_counts}\n",
    "sentences_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df[sentences_df.times_mentioned >=150].sort_values(by = 'rating', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best Thai dishes in SF\n",
    "sentences_df.to_csv('./best_dishes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Modelling to infer if beta coefficients are consistent with top dishes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new menu from previous data-set\n",
    "new_menu = ['fish cake',\n",
    "'roast duck',\n",
    "'samosa',\n",
    "'coconut ice cream',\n",
    "'tom ka',\n",
    "'egg rolls',\n",
    "'ribs',\n",
    "'egg roll',\n",
    "'pork belly',\n",
    "'fried chicken',\n",
    "'panang curry',\n",
    "'larb',\n",
    "'salmon',\n",
    "'calamari',\n",
    "'angel wings',\n",
    "'fried banana',\n",
    "'chicken satay',\n",
    "'tom kha',\n",
    "'pad kee mao',\n",
    "'pineapple fried rice',\n",
    "'prawns',\n",
    "'seafood',\n",
    "'spring rolls',\n",
    "'peanut sauce',\n",
    "'noodle soup',\n",
    "'pumpkin curry',\n",
    "'spring roll',\n",
    "'crab',\n",
    "'satay',\n",
    "'thai iced tea',\n",
    "'sticky rice',\n",
    "'papaya salad',\n",
    "'yellow curry',\n",
    "'papaya',\n",
    "'eggplant',\n",
    "'wings',\n",
    "'tom yum',\n",
    "'roti',\n",
    "'mango',\n",
    "'pad see ew',\n",
    "'green curry',\n",
    "'rolls',\n",
    "'basil',\n",
    "'coconut',\n",
    "'pork',\n",
    "'shrimp',\n",
    "'tofu',\n",
    "'beef',\n",
    "'fried rice',\n",
    "'salad',\n",
    "'noodles',\n",
    "'pad thai',\n",
    "'chicken',\n",
    "'curry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector(sentence, menu):\n",
    "    return [1*(dish in sentence) for dish in menu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_sentences = [sentence_to_vector(sentence, new_menu) for sentence in reviews_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.matrix(variable_sentences)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [TextBlob(x).sentiment.polarity for x in reviews_sentences]\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out beta coefficients\n",
    "data = {'coef' : lr.coef_,\n",
    "       'dish' : new_menu}\n",
    "df = pd.DataFrame(data)\n",
    "df.sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The beta coefficients are close to the top Thai dishes in San Francisco but inconsistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LassoCV, Lasso\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cv_r2(model):\n",
    "#     r2 = np.mean(cross_val_score(model, X, y,scoring=\"r2\",cv = 5))\n",
    "#     return(r2)\n",
    "# def lasso_selector(a):\n",
    "#     lasso_model = make_pipeline(StandardScaler(),LassoCV(max_iter=1e7, alphas = [a],cv=5)).fit(X, y)\n",
    "#     lasso_r2 = cv_r2(lasso_model).mean()\n",
    "#     return(lasso_r2)\n",
    "# lasso_alphas = [.0001, .0003, .0005, .0007, .0009,.01, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 20, 30, 50, 100]\n",
    "# lasso_scores = []\n",
    "# for alpha in lasso_alphas:\n",
    "#     score = lasso_selector(alpha)\n",
    "#     lasso_scores.append(score)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Analyzing our Alphas\n",
    "# plt.plot(lasso_alphas, lasso_scores, label='Lasso')\n",
    "# lasso_score_table = pd.DataFrame(lasso_scores, lasso_alphas, columns=['R2'])\n",
    "# lasso_score_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg = Lasso(alpha = 0.0001)\n",
    "# reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {'coef' : reg.coef_,\n",
    "#        'dish' : new_menu}\n",
    "# las_df = pd.DataFrame(data)\n",
    "# las_df.sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Top Dishes in Individual Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreating restaurant menu for finding top dishes in each restaurant\n",
    "sample_menu_rest=[\n",
    "'noodle soup','chicken noodle soup','beef noodle soup','pad see ew','pad kee mao','pad thai','fried rice','salad','papaya salad','satay','egg rolls','chicken','beef','fried chicken','roast duck','bbq pork','pork','roasted duck','panang curry','green curry','yellow curry','tom yum','tom kha','tom ka','thai iced tea','thai iced coffee',\n",
    "'imperial rolls','angel wings','wings','corn cakes','mango salad','panang beef','curry','basa fillet','tofu','pumpkin curry','coconut ice cream',\n",
    "'fried banana','sticky rice','pork belly','silver noodle','crab','calamari','fish cake','fish cakes','peanut sauce','samosa','catfish','pineapple fried rice','money bag','money bags','silver noodle','pad see you','larb','quail','prawns','fried prawn','fried prawns','shrimp','seafood','salmon','ribs','chicken noodle','beef noodle','roti','pad cha','spring rolls','fried egg','imperial roll','spring roll','egg roll','tuna tower','volcanic beef','sea bass','crab cake',\n",
    "'pad kee mow','massamam','lamb','drunken noodles'   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a restaurant function which takes in name of the restaurant and returns top 10 dishes\n",
    "def restaurant_food(a):\n",
    "    df = pd.read_csv('./yelp/yelp_thai_clean.csv')\n",
    "    #restrict to a Thai restaurant\n",
    "    df = df[df.restaurant == a]\n",
    "\n",
    "    #just grab rating and review text\n",
    "    df = df[['rating', 'text']]\n",
    "\n",
    "    #switch to lower case\n",
    "    df.text = df.text.apply(lambda x: x.lower())\n",
    "    \n",
    "    # this returns just those reviews that have the word\n",
    "    # in the text of the review\n",
    "    def subset_reviews(word, df):\n",
    "        return df[df.text.str.contains(word)]\n",
    "\n",
    "    # return avg rating of revies that contain dish\n",
    "    def avg_review_of_dish(item, df):\n",
    "        return subset_reviews(item, df).mean()\n",
    "\n",
    "    # return nuber of times dishes reviewed\n",
    "    def dish_count(item, df):\n",
    "        return subset_reviews(item, df).shape[0]\n",
    "    # split the reviews into sentences\n",
    "    reviews_sentences = ''.join(list(df.text)).split('.')\n",
    "\n",
    "    # create a dataframe with these sentences\n",
    "    data = {'text' : reviews_sentences}\n",
    "    sentences_df = pd.DataFrame(data)\n",
    "    sentences_df.text = sentences_df.text.apply(lambda x: x.lower())\n",
    "\n",
    "    # create column which is polarity of text\n",
    "    from textblob import TextBlob\n",
    "    sentences_df['polarity'] = sentences_df.text.apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    \n",
    "    dish_ratings = [avg_review_of_dish(item, sentences_df)[0] for item in sample_menu_rest]\n",
    "    dish_counts = [dish_count(item, sentences_df) for item in sample_menu_rest]\n",
    "    data = {'dish' : sample_menu_rest,\n",
    "           'rating' : dish_ratings,\n",
    "           'times_mentioned': dish_counts}\n",
    "    sentences_df = pd.DataFrame(data)\n",
    "    return sentences_df[sentences_df.times_mentioned >=9].sort_values(by = 'rating', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the best dishes in an individual restaurant\n",
    "restaurant_food(\"Osha Thai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the best dishes in an individual restaurant\n",
    "restaurant_food(\"Marnee Thai\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
